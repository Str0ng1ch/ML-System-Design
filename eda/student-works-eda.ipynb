{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-15T13:20:45.784991Z",
     "start_time": "2025-12-15T13:20:45.780398Z"
    }
   },
   "source": [
    "# ============================================================================\n",
    "# –§–ê–ô–õ: analysis_student_works.py\n",
    "# –î–ï–¢–ê–õ–¨–ù–´–ô EDA –ê–ù–ê–õ–ò–ó –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "DetectorFactory.seed = 0\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import textract\n",
    "\n",
    "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ü–ê–†–ê–ú–ï–¢–†–´ –ê–ù–ê–õ–ò–ó–ê\n",
    "# ============================================================================\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "COURSE_MATERIALS_PATH = Path('course_materials_detailed_analysis.csv')\n",
    "STUDENT_WORKS_DIR = Path('data/student-works')\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞\n",
    "STUDENT_WORK_TYPES = ['.pdf', '.docx', '.txt', '.doc']\n",
    "MAX_PAGES_PER_WORK = 30  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "\n",
    "print(\"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "print(f\"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç: {STUDENT_WORKS_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\n",
      "‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç: data/student-works\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:20:01.837634Z",
     "start_time": "2025-12-15T13:20:01.832298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# 2. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –†–ê–ó–ù–´–ú–ò –§–û–†–ú–ê–¢–ê–ú–ò\n",
    "# ============================================================================\n",
    "\n",
    "def extract_text_from_pdf(file_path, max_pages=MAX_PAGES_PER_WORK):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages[:max_pages]):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF {file_path.name}: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOCX {file_path.name}: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ TXT —Ñ–∞–π–ª–∞\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ TXT {file_path.name}: {e}\")\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "\n",
    "    if ext == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == '.docx':\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext == '.doc':\n",
    "        # –î–ª—è .doc –∏—Å–ø–æ–ª—å–∑—É–µ–º textract\n",
    "        try:\n",
    "            text = textract.process(file_path).decode('utf-8')\n",
    "            return text\n",
    "        except:\n",
    "            return extract_text_from_txt(file_path)\n",
    "    elif ext == '.txt':\n",
    "        return extract_text_from_txt(file_path)\n",
    "    else:\n",
    "        print(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏ —Å–æ–∑–¥–∞–Ω—ã\")"
   ],
   "id": "1b044eb06cf38a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏ —Å–æ–∑–¥–∞–Ω—ã\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:20:12.021927Z",
     "start_time": "2025-12-15T13:20:11.783969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# 3. –°–õ–û–í–ê–†–ò –ò –†–ï–°–£–†–°–´ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢\n",
    "# ============================================================================\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç\n",
    "STOPWORDS_RU = set(stopwords.words('russian'))\n",
    "STOPWORDS_EN = set(stopwords.words('english'))\n",
    "\n",
    "STUDENT_STOPWORDS = set()\n",
    "STUDENT_STOPWORDS.update(STOPWORDS_RU)\n",
    "STUDENT_STOPWORDS.update(STOPWORDS_EN)\n",
    "STUDENT_STOPWORDS.update([\n",
    "    # –û–±—â–∏–µ —Å–ª–æ–≤–∞\n",
    "    '—ç—Ç–æ', '–≤–æ—Ç', '—á—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–∞', '–Ω–æ', '–¥–∞', '–Ω–µ—Ç',\n",
    "    '–∏–ª–∏', '–ª–∏', '–∂–µ', '–±—ã', '–±', '–≤–µ–¥—å', '–≤—Ä–æ–¥–µ', '–¥–µ—Å–∫–∞—Ç—å', '–º–æ–ª',\n",
    "    '–Ω—É', '—Ç–æ', '–≤—Å–µ', '–≤—Å—ë', '–µ—â–µ', '–µ—â—ë', '—É–∂–µ', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ',\n",
    "    '–ª–∏–±–æ', '–Ω–∏–±—É–¥—å', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '—á–µ–π',\n",
    "    '—á—å—è', '—á—å—ë', '—á—å–∏', '—Å–∫–æ–ª—å–∫–æ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ',\n",
    "    '–∫–æ—Ç–æ—Ä—ã–µ', '–Ω–∏–∫—Ç–æ', '–Ω–∏—á—Ç–æ', '–Ω–µ–∫–∏–π', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–π', '–∫–∞–∫–æ–π-—Ç–æ',\n",
    "\n",
    "    # –°–ª–æ–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç\n",
    "    '—Ä–∞–±–æ—Ç–∞', '–∫—É—Ä—Å–æ–≤–∞—è', '—Ä–µ—Ñ–µ—Ä–∞—Ç', '—ç—Å—Å–µ', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–∞–Ω–∞–ª–∏–∑',\n",
    "    '–∏–∑—É—á–µ–Ω–∏–µ', '—Ü–µ–ª—å', '–∑–∞–¥–∞—á–∞', '–≤–≤–µ–¥–µ–Ω–∏–µ', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '–≤—ã–≤–æ–¥',\n",
    "    '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–º–µ—Ç–æ–¥', '–º–µ—Ç–æ–¥–∏–∫–∞', '—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π', '–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π',\n",
    "    '—Å—Ç—É–¥–µ–Ω—Ç', '–∞–≤—Ç–æ—Ä', '–Ω–∞–ø–∏—Å–∞–Ω–æ', '–ø—Ä–∏–≤–µ–¥–µ–Ω–æ', '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ',\n",
    "    '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–æ', '–∏–∑—É—á–µ–Ω–æ', '—Å–¥–µ–ª–∞–Ω–æ', '–ø–æ–∫–∞–∑–∞–Ω–æ',\n",
    "    '–≥–ª–∞–≤–∞', '–ø–∞—Ä–∞–≥—Ä–∞—Ñ', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞', '–ª–∏—Å—Ç',\n",
    "    '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '—Ñ–∞–∫—É–ª—å—Ç–µ—Ç', '–∫–∞—Ñ–µ–¥—Ä–∞', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å',\n",
    "    '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–Ω–∞—É—á–Ω—ã–π', '—É—á–µ–±–Ω—ã–π', '—É—á–µ–±–Ω–æ–µ', '–ø–æ—Å–æ–±–∏–µ',\n",
    "    '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–∏—Å—Ç–æ—á–Ω–∏–∫', '—Å—Å—ã–ª–∫–∞', '–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è', '—Å–ø–∏—Å–æ–∫',\n",
    "    '—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫', '–≥—Ä–∞—Ñ–∏–∫', '–¥–∏–∞–≥—Ä–∞–º–º–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',\n",
    "    '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ', '–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è', '—Ä–µ—Ü–µ–Ω–∑–∏—è', '–æ—Ç–∑—ã–≤'\n",
    "])\n",
    "\n",
    "# –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç\n",
    "EVALUATION_CRITERIA = {\n",
    "    'structure': ['–≤–≤–µ–¥–µ–Ω–∏–µ', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'],\n",
    "    'academic': ['–∏—Å—Ç–æ—á–Ω–∏–∫', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—Å—Å—ã–ª–∫–∞', '—Ü–∏—Ç–∞—Ç–∞', '—Å—Å—ã–ª–∫'],\n",
    "    'analysis': ['–∞–Ω–∞–ª–∏–∑', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–∏–∑—É—á–µ–Ω–∏–µ', '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ'],\n",
    "    'methodology': ['–º–µ—Ç–æ–¥', '–º–µ—Ç–æ–¥–∏–∫–∞', '–ø–æ–¥—Ö–æ–¥', '—Ç–µ–æ—Ä–∏—è'],\n",
    "    'results': ['—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–≤—ã–≤–æ–¥', '–∏—Ç–æ–≥', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ'],\n",
    "    'formatting': ['—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫', '–≥—Ä–∞—Ñ–∏–∫', '–¥–∏–∞–≥—Ä–∞–º–º–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']\n",
    "}\n",
    "\n",
    "# –û—Ü–µ–Ω–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "EVALUATION_METRICS = {\n",
    "    'min_words_for_essay': 1500,      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è —ç—Å—Å–µ\n",
    "    'min_words_for_coursework': 3000, # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –∫—É—Ä—Å–æ–≤–æ–π\n",
    "    'min_references': 5,              # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
    "    'max_special_chars': 0.15,        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤\n",
    "    'min_lexical_diversity': 0.3,     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\n",
    "}\n",
    "\n",
    "print(\"‚úÖ –°–ª–æ–≤–∞—Ä–∏ –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ],
   "id": "44ffe1f3b3d94486",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–ª–æ–≤–∞—Ä–∏ –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:20:18.837074Z",
     "start_time": "2025-12-15T13:20:18.817743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# 4. –ö–õ–ê–°–° –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢\n",
    "# ============================================================================\n",
    "\n",
    "class StudentWorksAnalyzer:\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º EDA\"\"\"\n",
    "\n",
    "    def __init__(self, works_dir, course_materials_path=None):\n",
    "        self.works_dir = Path(works_dir)\n",
    "        self.works_files = self._find_student_works()\n",
    "        self.results = []\n",
    "        self.all_text = \"\"\n",
    "        self.course_data = None\n",
    "\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å\n",
    "        if course_materials_path and Path(course_materials_path).exists():\n",
    "            self.course_data = pd.read_csv(course_materials_path)\n",
    "            print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞: {len(self.course_data)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "\n",
    "    def _find_student_works(self):\n",
    "        \"\"\"–ü–æ–∏—Å–∫ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\"\"\"\n",
    "        works = []\n",
    "        for ext in STUDENT_WORK_TYPES:\n",
    "            works.extend(list(self.works_dir.glob(f\"*{ext}\")))\n",
    "            works.extend(list(self.works_dir.glob(f\"**/*{ext}\")))  # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ\n",
    "\n",
    "        print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç: {len(works)}\")\n",
    "        return works\n",
    "\n",
    "    def analyze_language(self, text):\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        try:\n",
    "            return detect(text[:1000])\n",
    "        except LangDetectException:\n",
    "            return \"unknown\"\n",
    "\n",
    "    def calculate_basic_metrics(self, text):\n",
    "        \"\"\"–†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'total_chars': 0,\n",
    "                'total_words': 0,\n",
    "                'avg_word_length': 0,\n",
    "                'special_chars_ratio': 0,\n",
    "                'avg_sentence_length': 0,\n",
    "                'lexical_diversity': 0,\n",
    "                'readability_score': 0\n",
    "            }\n",
    "\n",
    "        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        total_chars = len(text)\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        total_words = len(words)\n",
    "\n",
    "        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞\n",
    "        avg_word_length = sum(len(word) for word in words) / max(total_words, 1)\n",
    "\n",
    "        # –î–æ–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        special_chars = len(re.findall(r'[^\\w\\s]', text))\n",
    "        special_chars_ratio = special_chars / max(total_chars, 1)\n",
    "\n",
    "        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        num_sentences = len([s for s in sentences if s.strip()])\n",
    "        avg_sentence_length = total_words / max(num_sentences, 1)\n",
    "\n",
    "        # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ\n",
    "        unique_words = set(words)\n",
    "        lexical_diversity = len(unique_words) / max(total_words, 1) if total_words > 0 else 0\n",
    "\n",
    "        # –û—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)\n",
    "        readability_score = (avg_word_length * 0.5) + (avg_sentence_length * 0.3)\n",
    "\n",
    "        return {\n",
    "            'total_chars': total_chars,\n",
    "            'total_words': total_words,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'special_chars_ratio': special_chars_ratio,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'lexical_diversity': lexical_diversity,\n",
    "            'readability_score': readability_score\n",
    "        }\n",
    "\n",
    "    def analyze_academic_quality(self, text):\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        results = {}\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "        for criterion, keywords in EVALUATION_CRITERIA.items():\n",
    "            count = 0\n",
    "            for keyword in keywords:\n",
    "                count += len(re.findall(keyword.lower(), text_lower))\n",
    "            results[f'criterion_{criterion}'] = count\n",
    "\n",
    "        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
    "        reference_patterns = [\n",
    "            r'\\[[\\d\\s,]+\\]',  # [1], [2, 3]\n",
    "            r'\\(\\d{4}\\)',     # (2020)\n",
    "            r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+\\s–∏\\s–¥—Ä\\.',  # –ò–≤–∞–Ω–æ–≤ –∏ –¥—Ä.\n",
    "            r'[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+\\set\\sal\\.',  # Ivanov et al.\n",
    "        ]\n",
    "\n",
    "        references_count = 0\n",
    "        for pattern in reference_patterns:\n",
    "            references_count += len(re.findall(pattern, text))\n",
    "\n",
    "        results['references_count'] = references_count\n",
    "\n",
    "        # –û—Ü–µ–Ω–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        unique_sentences = set(s.strip().lower() for s in sentences if len(s.strip()) > 20)\n",
    "        uniqueness_ratio = len(unique_sentences) / max(len(sentences), 1)\n",
    "        results['uniqueness_ratio'] = uniqueness_ratio\n",
    "\n",
    "        return results\n",
    "\n",
    "    def extract_key_phrases(self, text, top_n=15):\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        if not text:\n",
    "            return {}\n",
    "\n",
    "        text_lower = text.lower()\n",
    "        text_clean = re.sub(r'[^\\w\\s]', ' ', text_lower)\n",
    "        text_clean = re.sub(r'\\d+', ' ', text_clean)\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞\n",
    "        words = text_clean.split()\n",
    "        filtered_words = [\n",
    "            word for word in words\n",
    "            if (word not in STUDENT_STOPWORDS and\n",
    "                len(word) > 3 and\n",
    "                not word.isspace())\n",
    "        ]\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è (–±–∏–≥—Ä–∞–º–º—ã)\n",
    "        bigrams = []\n",
    "        for i in range(len(filtered_words) - 1):\n",
    "            bigram = f\"{filtered_words[i]} {filtered_words[i+1]}\"\n",
    "            if len(bigram.split()) == 2:\n",
    "                bigrams.append(bigram)\n",
    "\n",
    "        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É\n",
    "        word_freq = Counter(filtered_words)\n",
    "        bigram_freq = Counter(bigrams)\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        all_phrases = dict(word_freq.most_common(top_n))\n",
    "        all_phrases.update(dict(bigram_freq.most_common(top_n // 2)))\n",
    "\n",
    "        return dict(sorted(all_phrases.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "    def detect_work_type(self, text, filename):\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        # –ü–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞\n",
    "        if any(term in filename_lower for term in ['–∫—É—Ä—Å–æ–≤–∞—è', 'coursework', 'course_work']):\n",
    "            return '–∫—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞'\n",
    "        elif any(term in filename_lower for term in ['—ç—Å—Å–µ', 'essay']):\n",
    "            return '—ç—Å—Å–µ'\n",
    "        elif any(term in filename_lower for term in ['—Ä–µ—Ñ–µ—Ä–∞—Ç', 'report']):\n",
    "            return '—Ä–µ—Ñ–µ—Ä–∞—Ç'\n",
    "        elif any(term in filename_lower for term in ['–¥–∏–ø–ª–æ–º', 'thesis', '–¥–∏–ø–ª–æ–º–Ω–∞—è']):\n",
    "            return '–¥–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞'\n",
    "        elif any(term in filename_lower for term in ['–ø—Ä–æ–µ–∫—Ç', 'project']):\n",
    "            return '–ø—Ä–æ–µ–∫—Ç'\n",
    "\n",
    "        # –ü–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "        if '–∫—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞' in text_lower or '–∫—É—Ä—Å–æ–≤–æ–π –ø—Ä–æ–µ–∫—Ç' in text_lower:\n",
    "            return '–∫—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞'\n",
    "        elif '—ç—Å—Å–µ' in text_lower or 'essay' in text_lower:\n",
    "            return '—ç—Å—Å–µ'\n",
    "        elif '—Ä–µ—Ñ–µ—Ä–∞—Ç' in text_lower:\n",
    "            return '—Ä–µ—Ñ–µ—Ä–∞—Ç'\n",
    "        elif '–¥–∏–ø–ª–æ–º' in text_lower or '–≤—ã–ø—É—Å–∫–Ω–∞—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è' in text_lower:\n",
    "            return '–¥–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞'\n",
    "        elif '–ø—Ä–æ–µ–∫—Ç' in text_lower and '–∫—É—Ä—Å–æ–≤–æ–π' not in text_lower:\n",
    "            return '–ø—Ä–æ–µ–∫—Ç'\n",
    "\n",
    "        # –ü–æ –æ–±—ä–µ–º—É —Ç–µ–∫—Å—Ç–∞\n",
    "        words_count = len(re.findall(r'\\b\\w+\\b', text_lower))\n",
    "        if words_count > 5000:\n",
    "            return '–∫—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞ (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ)'\n",
    "        elif words_count > 1000:\n",
    "            return '—ç—Å—Å–µ (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ)'\n",
    "        else:\n",
    "            return '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'\n",
    "\n",
    "    def calculate_evaluation_score(self, metrics, academic_metrics):\n",
    "        \"\"\"–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ—á–Ω–æ–≥–æ –±–∞–ª–ª–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã\"\"\"\n",
    "        score = 0\n",
    "        max_score = 100\n",
    "\n",
    "        # 1. –û–±—ä–µ–º —Ä–∞–±–æ—Ç—ã (30 –±–∞–ª–ª–æ–≤)\n",
    "        words = metrics['total_words']\n",
    "        if words >= EVALUATION_METRICS['min_words_for_coursework']:\n",
    "            score += 30\n",
    "        elif words >= EVALUATION_METRICS['min_words_for_essay']:\n",
    "            score += 20\n",
    "        elif words >= 500:\n",
    "            score += 10\n",
    "\n",
    "        # 2. –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (20 –±–∞–ª–ª–æ–≤)\n",
    "        lexical_diversity = metrics['lexical_diversity']\n",
    "        if lexical_diversity >= 0.5:\n",
    "            score += 20\n",
    "        elif lexical_diversity >= EVALUATION_METRICS['min_lexical_diversity']:\n",
    "            score += 15\n",
    "        elif lexical_diversity >= 0.2:\n",
    "            score += 10\n",
    "        else:\n",
    "            score += 5\n",
    "\n",
    "        # 3. –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (30 –±–∞–ª–ª–æ–≤)\n",
    "        references = academic_metrics.get('references_count', 0)\n",
    "        if references >= 10:\n",
    "            score += 30\n",
    "        elif references >= EVALUATION_METRICS['min_references']:\n",
    "            score += 20\n",
    "        elif references >= 3:\n",
    "            score += 10\n",
    "\n",
    "        # 4. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã (20 –±–∞–ª–ª–æ–≤)\n",
    "        structure_score = 0\n",
    "        for key in ['criterion_structure', 'criterion_analysis', 'criterion_results']:\n",
    "            if academic_metrics.get(key, 0) > 0:\n",
    "                structure_score += 5\n",
    "\n",
    "        score += min(structure_score, 20)\n",
    "\n",
    "        return min(score, max_score)\n",
    "\n",
    "    def compare_with_course_materials(self, student_keywords, student_metrics):\n",
    "        \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∫—É—Ä—Å–∞\"\"\"\n",
    "        if self.course_data is None or self.course_data.empty:\n",
    "            return {\n",
    "                'similarity_score': 0,\n",
    "                'common_topics': [],\n",
    "                'course_coverage': 0\n",
    "            }\n",
    "\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞\n",
    "        course_keywords = {}\n",
    "        if 'top_keywords' in self.course_data.columns:\n",
    "            for keywords_str in self.course_data['top_keywords'].dropna():\n",
    "                try:\n",
    "                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "                    if isinstance(keywords_str, str):\n",
    "                        # –§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1(10), —Å–ª–æ–≤–æ2(5)\n",
    "                        pairs = keywords_str.split(', ')\n",
    "                        for pair in pairs:\n",
    "                            if '(' in pair and ')' in pair:\n",
    "                                word, count = pair.split('(')\n",
    "                                count = int(count.replace(')', ''))\n",
    "                                word = word.strip()\n",
    "                                course_keywords[word] = course_keywords.get(word, 0) + count\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "        common_keywords = set(student_keywords.keys()) & set(course_keywords.keys())\n",
    "        similarity_score = len(common_keywords) / max(len(set(student_keywords.keys())), 1)\n",
    "\n",
    "        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ —Ç–µ–º—ã\n",
    "        common_topics = []\n",
    "        for keyword in common_keywords:\n",
    "            student_freq = student_keywords.get(keyword, 0)\n",
    "            course_freq = course_keywords.get(keyword, 0)\n",
    "            common_topics.append({\n",
    "                'keyword': keyword,\n",
    "                'student_freq': student_freq,\n",
    "                'course_freq': course_freq\n",
    "            })\n",
    "\n",
    "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ —É —Å—Ç—É–¥–µ–Ω—Ç–∞\n",
    "        common_topics.sort(key=lambda x: x['student_freq'], reverse=True)\n",
    "\n",
    "        # –û—Ö–≤–∞—Ç –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞\n",
    "        total_course_keywords = len(course_keywords)\n",
    "        coverage = len(common_keywords) / max(total_course_keywords, 1)\n",
    "\n",
    "        return {\n",
    "            'similarity_score': similarity_score,\n",
    "            'common_topics': common_topics[:10],  # –¢–æ–ø-10 –æ–±—â–∏—Ö —Ç–µ–º\n",
    "            'course_coverage': coverage,\n",
    "            'common_keywords_count': len(common_keywords)\n",
    "        }\n",
    "\n",
    "    def analyze_all_works(self):\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìö –ê–ù–ê–õ–ò–ó –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for i, work_file in enumerate(self.works_files, 1):\n",
    "            print(f\"\\nüìù –†–∞–±–æ—Ç–∞ {i}/{len(self.works_files)}: {work_file.name}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n",
    "            text = extract_text_from_file(work_file)\n",
    "            self.all_text += text + \"\\n\"\n",
    "\n",
    "            if not text:\n",
    "                print(\"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω\")\n",
    "                continue\n",
    "\n",
    "            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "            metrics = self.calculate_basic_metrics(text)\n",
    "\n",
    "            # –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "            academic_metrics = self.analyze_academic_quality(text)\n",
    "\n",
    "            # –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã\n",
    "            key_phrases = self.extract_key_phrases(text, top_n=15)\n",
    "\n",
    "            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä–∞–±–æ—Ç—ã\n",
    "            work_type = self.detect_work_type(text, work_file.name)\n",
    "\n",
    "            # –Ø–∑—ã–∫ —Ä–∞–±–æ—Ç—ã\n",
    "            language = self.analyze_language(text)\n",
    "\n",
    "            # –û—Ü–µ–Ω–æ—á–Ω—ã–π –±–∞–ª–ª\n",
    "            evaluation_score = self.calculate_evaluation_score(metrics, academic_metrics)\n",
    "\n",
    "            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∫—É—Ä—Å–∞\n",
    "            comparison = self.compare_with_course_materials(key_phrases, metrics)\n",
    "\n",
    "            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–π —Ä–∞–±–æ—Ç—ã\n",
    "            print(f\"  –¢–∏–ø —Ä–∞–±–æ—Ç—ã: {work_type}\")\n",
    "            print(f\"  –Ø–∑—ã–∫: {language}\")\n",
    "            print(f\"  –û–±—ä–µ–º: {metrics['total_words']:,} —Å–ª–æ–≤\")\n",
    "            print(f\"  –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {metrics['lexical_diversity']:.2%}\")\n",
    "            print(f\"  –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {academic_metrics.get('references_count', 0)}\")\n",
    "            print(f\"  –û—Ü–µ–Ω–æ—á–Ω—ã–π –±–∞–ª–ª: {evaluation_score}/100\")\n",
    "            print(f\"  –°—Ö–æ–¥—Å—Ç–≤–æ —Å –∫—É—Ä—Å–æ–º: {comparison['similarity_score']:.2%}\")\n",
    "\n",
    "            if key_phrases:\n",
    "                top_phrases = list(key_phrases.items())[:5]\n",
    "                phrases_str = ', '.join([f'{k}({v})' for k, v in top_phrases])\n",
    "                print(f\"  –¢–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑: {phrases_str}\")\n",
    "\n",
    "            if comparison['common_topics']:\n",
    "                common_str = ', '.join([f\"{item['keyword']}\" for item in comparison['common_topics'][:3]])\n",
    "                print(f\"  –û–±—â–∏–µ —Ç–µ–º—ã —Å –∫—É—Ä—Å–æ–º: {common_str}\")\n",
    "\n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            self.results.append({\n",
    "                'file_name': work_file.name,\n",
    "                'file_extension': work_file.suffix,\n",
    "                'file_size_mb': work_file.stat().st_size / (1024 * 1024),\n",
    "                'work_type': work_type,\n",
    "                'language': language,\n",
    "                'total_chars': metrics['total_chars'],\n",
    "                'total_words': metrics['total_words'],\n",
    "                'avg_word_length': metrics['avg_word_length'],\n",
    "                'special_chars_ratio': metrics['special_chars_ratio'],\n",
    "                'avg_sentence_length': metrics['avg_sentence_length'],\n",
    "                'lexical_diversity': metrics['lexical_diversity'],\n",
    "                'readability_score': metrics['readability_score'],\n",
    "                'key_phrases_count': len(key_phrases),\n",
    "                'top_key_phrases': dict(list(key_phrases.items())[:10]),\n",
    "                'references_count': academic_metrics.get('references_count', 0),\n",
    "                'structure_score': sum(academic_metrics.get(f'criterion_{k}', 0)\n",
    "                                     for k in ['structure', 'analysis', 'results']),\n",
    "                'uniqueness_ratio': academic_metrics.get('uniqueness_ratio', 0),\n",
    "                'evaluation_score': evaluation_score,\n",
    "                'similarity_with_course': comparison['similarity_score'],\n",
    "                'course_coverage': comparison['course_coverage'],\n",
    "                'common_keywords_count': comparison['common_keywords_count'],\n",
    "                'top_common_topics': [item['keyword'] for item in comparison['common_topics'][:5]]\n",
    "            })\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        works_df = pd.DataFrame(self.results)\n",
    "\n",
    "        if not works_df.empty:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"‚úÖ –ê–ù–ê–õ–ò–ó –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢ –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "            print(f\"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ä–∞–±–æ—Ç: {len(works_df)}\")\n",
    "            print(\"=\"*80)\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã\")\n",
    "\n",
    "        return works_df\n",
    "\n",
    "print(\"‚úÖ –ö–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!\")"
   ],
   "id": "a375cf80ddf93ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ö–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:22:35.220642Z",
     "start_time": "2025-12-15T13:22:35.154581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# 5. –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –ò –û–°–ù–û–í–ù–û–ô EDA\n",
    "# ============================================================================\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä\n",
    "if STUDENT_WORKS_DIR.exists():\n",
    "    analyzer = StudentWorksAnalyzer(STUDENT_WORKS_DIR, COURSE_MATERIALS_PATH)\n",
    "    works_df = analyzer.analyze_all_works()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {STUDENT_WORKS_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
    "    works_df = pd.DataFrame()\n",
    "\n",
    "if not works_df.empty:\n",
    "    # ============================================================================\n",
    "    # 6. –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–ú –†–ê–ë–û–¢–ê–ú\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–ú –†–ê–ë–û–¢–ê–ú\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    print(\"\\nüìà –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"–í—Å–µ–≥–æ —Ä–∞–±–æ—Ç: {len(works_df)}\")\n",
    "    print(f\"–û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {works_df['total_words'].sum():,} —Å–ª–æ–≤\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º —Ä–∞–±–æ—Ç—ã: {works_df['total_words'].mean():,.0f} —Å–ª–æ–≤\")\n",
    "    print(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π –æ–±—ä–µ–º: {works_df['total_words'].median():,.0f} —Å–ª–æ–≤\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {works_df['evaluation_score'].mean():.1f}/100\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫—É—Ä—Å–æ–º: {works_df['similarity_with_course'].mean():.2%}\")\n",
    "\n",
    "    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º —Ä–∞–±–æ—Ç\n",
    "    if 'work_type' in works_df.columns:\n",
    "        print(f\"\\nüìö –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –¢–ò–ü–ê–ú –†–ê–ë–û–¢:\")\n",
    "        print(\"-\" * 40)\n",
    "        type_counts = works_df['work_type'].value_counts()\n",
    "        for work_type, count in type_counts.items():\n",
    "            percentage = (count / len(works_df)) * 100\n",
    "            print(f\"  {work_type}: {count} —Ä–∞–±–æ—Ç ({percentage:.1f}%)\")\n",
    "\n",
    "    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º\n",
    "    if 'language' in works_df.columns:\n",
    "        print(f\"\\nüåç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –Ø–ó–´–ö–ê–ú:\")\n",
    "        print(\"-\" * 40)\n",
    "        lang_counts = works_df['language'].value_counts()\n",
    "        for lang, count in lang_counts.items():\n",
    "            percentage = (count / len(works_df)) * 100\n",
    "            print(f\"  {lang}: {count} —Ä–∞–±–æ—Ç ({percentage:.1f}%)\")\n",
    "\n",
    "    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    print(f\"\\nüéØ –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –†–ê–ë–û–¢:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∏\n",
    "    evaluation_stats = works_df['evaluation_score'].describe()\n",
    "    print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {evaluation_stats['min']:.1f}\")\n",
    "    print(f\"  –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {evaluation_stats['mean']:.1f}\")\n",
    "    print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {evaluation_stats['max']:.1f}\")\n",
    "\n",
    "    # –õ—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ —Ä–∞–±–æ—Ç—ã\n",
    "    if len(works_df) > 0:\n",
    "        best_work = works_df.loc[works_df['evaluation_score'].idxmax()]\n",
    "        worst_work = works_df.loc[works_df['evaluation_score'].idxmin()]\n",
    "\n",
    "        print(f\"\\nüèÜ –õ–£–ß–®–ê–Ø –†–ê–ë–û–¢–ê:\")\n",
    "        print(f\"  –§–∞–π–ª: {best_work['file_name']}\")\n",
    "        print(f\"  –û—Ü–µ–Ω–∫–∞: {best_work['evaluation_score']:.1f}/100\")\n",
    "        print(f\"  –û–±—ä–µ–º: {best_work['total_words']:,} —Å–ª–æ–≤\")\n",
    "\n",
    "        print(f\"\\n‚ö†Ô∏è –•–£–î–®–ê–Ø –†–ê–ë–û–¢–ê:\")\n",
    "        print(f\"  –§–∞–π–ª: {worst_work['file_name']}\")\n",
    "        print(f\"  –û—Ü–µ–Ω–∫–∞: {worst_work['evaluation_score']:.1f}/100\")\n",
    "        print(f\"  –û–±—ä–µ–º: {worst_work['total_words']:,} —Å–ª–æ–≤\")\n",
    "\n",
    "    # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –∫—É—Ä—Å–æ–º\n",
    "    print(f\"\\nüìö –°–•–û–î–°–¢–í–û –° –ú–ê–¢–ï–†–ò–ê–õ–ê–ú–ò –ö–£–†–°–ê:\")\n",
    "    print(\"-\" * 40)\n",
    "    similarity_stats = works_df['similarity_with_course'].describe()\n",
    "    print(f\"  –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity_stats['mean']:.2%}\")\n",
    "    print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity_stats['min']:.2%}\")\n",
    "    print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity_stats['max']:.2%}\")\n",
    "\n",
    "    # –†–∞–±–æ—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º\n",
    "    high_similarity = works_df[works_df['similarity_with_course'] > 0.5]\n",
    "    if len(high_similarity) > 0:\n",
    "        print(f\"  –†–∞–±–æ—Ç —Å –≤—ã—Å–æ–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º (>50%): {len(high_similarity)}\")\n",
    "\n",
    "    # –†–∞–±–æ—Ç—ã —Å –Ω–∏–∑–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º\n",
    "    low_similarity = works_df[works_df['similarity_with_course'] < 0.1]\n",
    "    if len(low_similarity) > 0:\n",
    "        print(f\"  –†–∞–±–æ—Ç —Å –Ω–∏–∑–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º (<10%): {len(low_similarity)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ –°–í–û–î–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "    print(\"=\"*80)"
   ],
   "id": "c53c02a2cc3b262a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç: 16\n",
      "‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "\n",
      "================================================================================\n",
      "üìö –ê–ù–ê–õ–ò–ó –°–¢–£–î–ï–ù–ß–ï–°–ö–ò–• –†–ê–ë–û–¢\n",
      "================================================================================\n",
      "\n",
      "üìù –†–∞–±–æ—Ç–∞ 1/16: –≠—Å—Å–µ_—Å–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–∏ÃÜ_–≥–æ—Ä–æ–¥_–ü—É—à–∫–∞—Ä–µ–≤–∞_–î–∞—Ä—å—è.docx\n",
      "----------------------------------------\n",
      "  –¢–∏–ø —Ä–∞–±–æ—Ç—ã: —ç—Å—Å–µ\n",
      "  –Ø–∑—ã–∫: ru\n",
      "  –û–±—ä–µ–º: 1,347 —Å–ª–æ–≤\n",
      "  –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: 56.27%\n",
      "  –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: 0\n",
      "  –û—Ü–µ–Ω–æ—á–Ω—ã–π –±–∞–ª–ª: 30/100\n",
      "  –°—Ö–æ–¥—Å—Ç–≤–æ —Å –∫—É—Ä—Å–æ–º: 0.00%\n",
      "  –¢–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑: –≥–æ—Ä–æ–¥(15), –≥–æ—Ä–æ–¥–∞(9), –∂–∏–∑–Ω–∏(6), –ø–æ–Ω–∏–º–∞–ª–∞(6), –∫—Ä—É–≥(4)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'common_keywords_count'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m STUDENT_WORKS_DIR.exists():\n\u001B[32m      7\u001B[39m     analyzer = StudentWorksAnalyzer(STUDENT_WORKS_DIR, COURSE_MATERIALS_PATH)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     works_df = \u001B[43manalyzer\u001B[49m\u001B[43m.\u001B[49m\u001B[43manalyze_all_works\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     10\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSTUDENT_WORKS_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 373\u001B[39m, in \u001B[36mStudentWorksAnalyzer.analyze_all_works\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    348\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  –û–±—â–∏–µ —Ç–µ–º—ã —Å –∫—É—Ä—Å–æ–º: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcommon_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    350\u001B[39m     \u001B[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\u001B[39;00m\n\u001B[32m    351\u001B[39m     \u001B[38;5;28mself\u001B[39m.results.append({\n\u001B[32m    352\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mfile_name\u001B[39m\u001B[33m'\u001B[39m: work_file.name,\n\u001B[32m    353\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mfile_extension\u001B[39m\u001B[33m'\u001B[39m: work_file.suffix,\n\u001B[32m    354\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mfile_size_mb\u001B[39m\u001B[33m'\u001B[39m: work_file.stat().st_size / (\u001B[32m1024\u001B[39m * \u001B[32m1024\u001B[39m),\n\u001B[32m    355\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mwork_type\u001B[39m\u001B[33m'\u001B[39m: work_type,\n\u001B[32m    356\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mlanguage\u001B[39m\u001B[33m'\u001B[39m: language,\n\u001B[32m    357\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtotal_chars\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mtotal_chars\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    358\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtotal_words\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mtotal_words\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    359\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mavg_word_length\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mavg_word_length\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    360\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mspecial_chars_ratio\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mspecial_chars_ratio\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    361\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mavg_sentence_length\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mavg_sentence_length\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    362\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mlexical_diversity\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mlexical_diversity\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    363\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mreadability_score\u001B[39m\u001B[33m'\u001B[39m: metrics[\u001B[33m'\u001B[39m\u001B[33mreadability_score\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    364\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mkey_phrases_count\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28mlen\u001B[39m(key_phrases),\n\u001B[32m    365\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtop_key_phrases\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mlist\u001B[39m(key_phrases.items())[:\u001B[32m10\u001B[39m]),\n\u001B[32m    366\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mreferences_count\u001B[39m\u001B[33m'\u001B[39m: academic_metrics.get(\u001B[33m'\u001B[39m\u001B[33mreferences_count\u001B[39m\u001B[33m'\u001B[39m, \u001B[32m0\u001B[39m),\n\u001B[32m    367\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mstructure_score\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28msum\u001B[39m(academic_metrics.get(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mcriterion_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m, \u001B[32m0\u001B[39m)\n\u001B[32m    368\u001B[39m                              \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mstructure\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33manalysis\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mresults\u001B[39m\u001B[33m'\u001B[39m]),\n\u001B[32m    369\u001B[39m         \u001B[33m'\u001B[39m\u001B[33muniqueness_ratio\u001B[39m\u001B[33m'\u001B[39m: academic_metrics.get(\u001B[33m'\u001B[39m\u001B[33muniqueness_ratio\u001B[39m\u001B[33m'\u001B[39m, \u001B[32m0\u001B[39m),\n\u001B[32m    370\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mevaluation_score\u001B[39m\u001B[33m'\u001B[39m: evaluation_score,\n\u001B[32m    371\u001B[39m         \u001B[33m'\u001B[39m\u001B[33msimilarity_with_course\u001B[39m\u001B[33m'\u001B[39m: comparison[\u001B[33m'\u001B[39m\u001B[33msimilarity_score\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    372\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mcourse_coverage\u001B[39m\u001B[33m'\u001B[39m: comparison[\u001B[33m'\u001B[39m\u001B[33mcourse_coverage\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mcommon_keywords_count\u001B[39m\u001B[33m'\u001B[39m: \u001B[43mcomparison\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcommon_keywords_count\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m,\n\u001B[32m    374\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtop_common_topics\u001B[39m\u001B[33m'\u001B[39m: [item[\u001B[33m'\u001B[39m\u001B[33mkeyword\u001B[39m\u001B[33m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m comparison[\u001B[33m'\u001B[39m\u001B[33mcommon_topics\u001B[39m\u001B[33m'\u001B[39m][:\u001B[32m5\u001B[39m]]\n\u001B[32m    375\u001B[39m     })\n\u001B[32m    377\u001B[39m \u001B[38;5;66;03m# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\u001B[39;00m\n\u001B[32m    378\u001B[39m works_df = pd.DataFrame(\u001B[38;5;28mself\u001B[39m.results)\n",
      "\u001B[31mKeyError\u001B[39m: 'common_keywords_count'"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
