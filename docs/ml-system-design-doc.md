# ML System Design Doc - [RU]

## Дизайн ML системы - Jane AI Assistant MVP v1

### 1. Цели и предпосылки

#### 1.1. Зачем идем в разработку продукта?

- **Бизнес-цель**: повысить глубину и академическую глубину студенческих работ по урбанистике, снизив при этом
  нагрузку на преподавателей по рутинной проверке и выдаче однотипных комментариев путем внедрения AI-ассистента,
  выдающего рекомендации по студенческим работам.
- **Почему станет лучше, чем сейчас, от использования ML**: AI-ассистент будет предоставлять быструю и содержательную
  обратную связь, основанную на материалах курса, побуждающую студентов работать с первоисточниками, что приведет к
  качественному улучшению их работ.
  Преподаватель освобождается от рутинных задач и фокусируется на сложных кейсах, либо на других задачах.
- **Что будем считать успехом итерации с точки зрения бизнеса**: увеличение количества релевантных ссылок на
  академические источники в финальных работах студентов после получения фидбека от AI-ассистента, а также положительные
  отзывы от пилотной группы студентов и преподавателя о полезности и релевантности рекомендаций.

#### 1.2. Бизнес-требования и ограничения

- **Бизнес-требования**: реализовать и внедрить AI-ассистента, который анализирует
  тексты студенческих работ и дает обратную связь в стиле Джейн Джекобс, рекомендуя конкретные концепции и источники из
  учебной программы курса.
- **Бизнес-ограничения**:
    1. время разработки ~45-60 дней;
    2. интеграция в привычную, удобную и доступную для студентов среду;
    3. обеспечение анонимности и конфиденциальности студенческих работ;
    4. устойчивость к попыткам злоупотребления (prompt-инъекции, спам);
    5. возможность отправки работ в разных форматах.
- **Что мы ожидаем от конкретной итерации**: рабочий прототип, способный анализировать текстовые файлы и давать
  содержательные ответы, основанные на предоставленной базе знаний - программе курса.
- **Описание бизнес-процесса пилота**: преподаватель на семинарах дает домашнее задание в виде эссе и рекомендует
  воспользоваться AI-ассистентом для получения более высоких баллов за работу -> студент загружает черновик
  работы в AI-ассистента -> получает анализ и рекомендации -> дорабатывает работу на основе фидбека -> сдает финальную
  версию преподавателю -> преподаватель проверяет улучшенную версию работы.
- **Критерии успешного пилота**: ассистент активно используется студентами, его рекомендации воспринимаются как полезные
  и приводят к конкретным улучшениям в работах. Преподаватель подтверждает снижение нагрузки на проверку
  и улучшение качества работ.

#### 1.3. Что входит в скоуп проекта/итерации, что не входит

- **Входит в скоуп**:
    1. разработка Telegram-бота как поверхности взаимодействия с пользователем;
    2. создание backend для обработки запросов и вызова LLM;
    3. реализация RAG для индексации предоставленных материалов курса;
    4. интеграция с Yandex LLM API для генерации ответов;
    5. внедрение базовой системы безопасности (защита от prompt-инъекций и вредоносных файлов, анонимизация данных).
- **Что не будет закрыто**:
    1. разработка веб-интерфейса;
    2. интеграция с академическими системами университета.
- **Описание результата с точки зрения качества кода и воспроизводимости решения**: код модульный, задокументирован,
  упакован в Docker-контейнеры для обеспечения воспроизводимости, по возможности обеспечение 99% доступности системы.
- **Описание планируемого технического долга**: живем без долгов :)

#### 1.4. Предпосылки решения <small> (Описание всех общих предпосылок решения, используемых в системе – с обоснованием от запроса бизнеса: какие блоки данных используем, горизонт прогноза, гранулярность модели, и др.) </small>

- **Блоки данных**: используется предоставленная преподавателем литература курса, составляющая базу знаний
  AI-ассистента.
- **Горизонт прогноза**: в NLP/LLM задачах зависит от модели и от ее количества предсказываемых следующих токенов.
- **Гранулярность модели**: модель работает на уровне отдельно взятой студенческой работы, например эссе.

### 2. Методология

#### 2.1. Постановка задачи

- **Что делаем с технической точки зрения**: задача анализа текста (Text Analysis) и интеллектуального поиска
  информации (Information Retrieval) с последующей генерацией текстового ответа (Text Generation). По сути, это система
  вопросно-ответная система (QA) основанная на закрытом наборе документов.

#### 2.2. Блок-схема решения

![img.png](flowchart.png)

### 3. Этапы решения задачи

#### Этап 1 - Подготовка данных и базы знаний

#### Данные и сущности

На данном этапе мы формируем базу знаний для RAG (Retrieval Augmented Generation) системы. Данные представляют собой
учебные материалы курса по урбанистике.

| Название данных              | Есть ли данные в компании (если да, название источника/витрин) | Требуемый ресурс для получения данных (какие роли нужны) | Проверено ли качество данных (да, нет) | Объем данных              |
|------------------------------|----------------------------------------------------------------|----------------------------------------------------------|----------------------------------------|---------------------------|
| PDF-файлы учебных материалов | Локальная директория `data/books`                              | Data Engineer / Data Scientist                           | +                                      | 9 PDF-файлов (~140K слов) |
| Аналитические результаты     | DataFrame `books_df` с результатами анализа                    | Data Scientist                                           | +                                      | 9 записей, 20+ метрик     |

#### Выявленные проблемы и риски:

1. **Объем данных ограничен** - 9 документов (140K слов) может быть недостаточно для полноценной RAG-системы. Решение -
   запросить у преподавателя больше данных при необходимости.
2. **Смещение в концепциях** - большинство предоставленной литературы относится к концепции экологического подхода
3. **Возможно содержание конфиденциальной информации** - в работах студентов могут встречаться ФИО и другая персональная
   информация

#### Процесс генерации и подготовки данных:

1. **Сбор данных**: PDF-файлы предоставляются преподавателем курса -> процесс нерегулярный, осуществляется в ручном
   формате по запросу
2. **Предобработка**:
    - Извлечение текста из PDF
    - Очистка от артефактов (стоп-слова, сцеп-символы и т.д.)
    - Разбиение на семантические чанки с перекрытием
3. **Индексация**:
    - Векторизация чанков, например с помощью `sentence-transformers`
    - Построение векторного индекса (FAISS)
    - Добавление метаданных (источник, тема, авторы)

#### Необходимый результат этапа:

- Векторная база знаний с API поиска
- Документация по структуре данных
- Успешно загруженные векторизованные чанки в векторную БД

#### Этап 2 - Подготовка прогнозных моделей

#### Постановка задачи в контексте генерации рекомендаций

**Что мы прогнозируем**: Текст рекомендаций по студенческим работам на основе:

1. Семантического содержания работы студента
2. Релевантных материалов из базы знаний
3. Контекста учебного курса

**Уточнение термина "прогнозная модель"**: В контексте генеративного AI, мы прогнозируем последовательность токенов (
текст), который будет максимально релевантен запросу и контексту.

#### ML-метрики и функции потерь

##### Для задачи поиска (Retrieval) - бейзлайн:

**Метрики**:

1. **Recall** - доля релевантных документов среди top-k результатов
    - Обоснование: В RAG-системе достаточно найти несколько релевантных фрагментов
2. **Mean Average Precision (MAP)** - средняя точность по всем релевантным документам
    - Обоснование: Важно не только найти релевантные документы, но и правильно их ранжировать

##### Для задачи поиска (Retrieval) - бейзлайн:

1. **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation):
    - Обоснование: Оценка лексического перекрытия с эталонными ответами
2. **Human Evaluation Metrics** - основа:
    - Relevance: Релевантность рекомендаций теме работы (1-5)
    - Usefulness: Практическая полезность рекомендаций (1-5)

##### Схема ML-валидации

1. **Вложенная кросс-валидация** для оптимизации гиперпараметров
2. **Временное разделение** для имитации реального использования

#### Бейзлайн

1. **LLM** без хорошего промт-инжиниринга - предпочтительнее
2. **Без LLM** путем составления шаблона и семантического поиска с использованием TF-IDF<br>

#### Стратегии дальнейшего развития решения

1. **Улучшение поиска**
    - Использование более качественных и мощных моделей
2. **Улучшение генерации**
    - Улучшение промпт-инжиниринга
    - Fine-tuning
    - Few-shot learning (Например примеры хороших рекомендаций в промпте)

#### Оптимизация гиперпараметров (список параметров для оптимизации):

1. chunk_size
2. chunk_overlap (пересечение чанков)
3. top_k_retrieval
4. temperature
5. max_tokens

#### Анализ и интерпретация работы модели

1. **Анализ ошибок поиска**
    - Почему нерелевантный документ был найден?
    - Почему релевантный документ не был найден?
2. **Анализ генерации**
    - Проверка на галлюцинации
    - Анализ структуры рекомендаций + семантический анализ
3. **Сбор обратной связи** от преподавателя и студентов

#### Риски данного этапа:

1. **Галлюцинации LLM** - самый важный и вероятный риск, однако есть методы снижения влияния и частоты появления
2. **Низкое качество поиска** - риск с высоким влияением, там как от него зависит последующая генерация ответа
3. **Смещение в рекомендациях** из-за смещения в концепциях (в первоначальных данных)

#### Необходимый результат этапа:

1. **Бейзлайн и MVP** модель + сравнение метрик
2. **Метрики качества**
    - Recall
    - Время ответа
    - Human evaluation
3. **Документация** по данным, исходному коду моделей, архитектуре моделей, анализу ошибок

### 3. Полезная информация по проекту

#### 3.1 Черновая архитектура

![img.png](arhitecture.png)
